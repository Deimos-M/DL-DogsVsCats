{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a small convent  from scratch\n",
    "\n",
    "* Dogs vs cats dataset :- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vipul\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D , Activation , Dropout , Flatten ,Dense\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dimensions of images\n",
    "image_width,image_height=256,256\n",
    "\n",
    "train_data_dir=r\"C:\\Users\\**\\Desktop\\jupyter\\DL\\CNN egs\\Data\\train\"\n",
    "valid_data_dir=r\"C:\\Users\\**\\Desktop\\jupyter\\DL\\CNN egs\\Data\\validation\"\n",
    "no_train_samples=2000\n",
    "no_valid_samples=600\n",
    "epochs=10\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#checking for dimension format of keras's backend\n",
    "if k.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, image_width, image_height)\n",
    "else:\n",
    "    input_shape = (image_width, image_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a small network \n",
    "model=Sequential()\n",
    "\n",
    "#1st convolution layer\n",
    "#note: no. of channels of convolution layer is equal to the no. of channels of input image which is calculated automatically.\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "#1st dense layer\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 125, 125, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                3686464   \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,715,169\n",
      "Trainable params: 3,715,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data augmentation configuration on training images\n",
    "train_datagen= ImageDataGenerator(    \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# data augmentation configuration on validation images\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flow_from_directory\n",
    "* directory: Path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator.\n",
    "* classes: Optional list of class subdirectories (e.g. ['dogs', 'cats']). Default: None. If not provided, the list of classes will be automatically inferred from the subdirectory names/structure under directory, where each subdirectory will be treated as a different class (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices.\n",
    "* Returns : A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 55s 885ms/step - loss: 1.6427 - acc: 0.4894 - val_loss: 0.6766 - val_acc: 0.5503\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 53s 859ms/step - loss: 0.7098 - acc: 0.5368 - val_loss: 0.6687 - val_acc: 0.5972\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 53s 857ms/step - loss: 0.7045 - acc: 0.5565 - val_loss: 0.6718 - val_acc: 0.5868\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 52s 839ms/step - loss: 0.6831 - acc: 0.5892 - val_loss: 0.6799 - val_acc: 0.5417\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 50s 805ms/step - loss: 0.6991 - acc: 0.5635 - val_loss: 0.6677 - val_acc: 0.5764\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 52s 833ms/step - loss: 0.6835 - acc: 0.5867 - val_loss: 0.6812 - val_acc: 0.5764\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 52s 831ms/step - loss: 0.6611 - acc: 0.5983 - val_loss: 0.6712 - val_acc: 0.6233\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 51s 825ms/step - loss: 0.6719 - acc: 0.6033 - val_loss: 0.6568 - val_acc: 0.6233\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 50s 814ms/step - loss: 0.6524 - acc: 0.6129 - val_loss: 0.6375 - val_acc: 0.6424\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 51s 822ms/step - loss: 0.6526 - acc: 0.6109 - val_loss: 0.6386 - val_acc: 0.6476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213905d8e80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch= no_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=no_valid_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** :-with this less data of 2000 training images and 3.7M trainable parameters if we use different activation functions we'll get following  validation accuracy within almost 10mins. <br>\n",
    "1) relu -51%<br>\n",
    "2) tanh -49%<br>\n",
    "3) sigmoid -65%<br>\n",
    "* Idealy tanh should perform better in binary classification problems in **output layer** over sigmoid but here as we can see  sigmoid is performing better , so this we cannot guess just from theory part , it is based(almost) on the data and trial-n-error method .\n",
    "* If you inc. no. of images you'll definitely get better results and another way is to use bottleneck features which is explained in another notebook(2nd_convNet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the final model\n",
    "model.save(\"1st_convNet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
